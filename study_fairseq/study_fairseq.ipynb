{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "Our Encoder will embed the tokens in the source sentence, feed them to a `torch.nn.LSTM` and return the final hidden state. To create our encoder save the following in a new file named fairseq/models/simple_lstm.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from fairseq import utils\n",
    "from fairseq.models import FairseqEncoder\n",
    "\n",
    "class SimpleLSTMEncoder(FairseqEncoder):\n",
    "\n",
    "    def __init__(\n",
    "        self, args, dictionary, embed_dim=128, hidden_dim=128, dropout=0.1,\n",
    "    ):\n",
    "        super().__init__(dictionary)\n",
    "        self.args = args\n",
    "\n",
    "        # Our encoder will embed the inputs before feeding them to the LSTM.\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            num_embeddings=len(dictionary),\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=dictionary.pad(),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # We'll use a single-layer, unidirectional LSTM for simplicity.\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            bidirectional=False,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, src_tokens, src_lengths):\n",
    "        # The inputs to the ``forward()`` function are determined by the\n",
    "        # Task, and in particular the ``'net_input'`` key in each\n",
    "        # mini-batch. We discuss Tasks in the next tutorial, but for now just\n",
    "        # know that *src_tokens* has shape `(batch, src_len)` and *src_lengths*\n",
    "        # has shape `(batch)`.\n",
    "\n",
    "        # Note that the source is typically padded on the left. This can be\n",
    "        # configured by adding the `--left-pad-source \"False\"` command-line\n",
    "        # argument, but here we'll make the Encoder handle either kind of\n",
    "        # padding by converting everything to be right-padded.\n",
    "        if self.args.left_pad_source:\n",
    "            # Convert left-padding to right-padding.\n",
    "            src_tokens = utils.convert_padding_direction(\n",
    "                src_tokens,\n",
    "                padding_idx=self.dictionary.pad(),\n",
    "                left_to_right=True\n",
    "            )\n",
    "\n",
    "        # Embed the source.\n",
    "        x = self.embed_tokens(src_tokens)\n",
    "\n",
    "        # Apply dropout.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pack the sequence into a PackedSequence object to feed to the LSTM.\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, src_lengths, batch_first=True)\n",
    "\n",
    "        # Get the output from the LSTM.\n",
    "        _outputs, (final_hidden, _final_cell) = self.lstm(x)\n",
    "\n",
    "        # Return the Encoder's output. This can be any object and will be\n",
    "        # passed directly to the Decoder.\n",
    "        return {\n",
    "            # this will have shape `(bsz, hidden_dim)`\n",
    "            'final_hidden': final_hidden.squeeze(0),\n",
    "        }\n",
    "\n",
    "    # Encoders are required to implement this method so that we can rearrange\n",
    "    # the order of the batch elements during inference (e.g., beam search).\n",
    "    def reorder_encoder_out(self, encoder_out, new_order):\n",
    "        \"\"\"\n",
    "        Reorder encoder output according to `new_order`.\n",
    "\n",
    "        Args:\n",
    "            encoder_out: output from the ``forward()`` method\n",
    "            new_order (LongTensor): desired order\n",
    "\n",
    "        Returns:\n",
    "            `encoder_out` rearranged according to `new_order`\n",
    "        \"\"\"\n",
    "        final_hidden = encoder_out['final_hidden']\n",
    "        return {\n",
    "            'final_hidden': final_hidden.index_select(0, new_order),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Our Decoder will predict the next word, conditioned on the Encoder’s final hidden state and an embedded representation of the previous target word – which is sometimes called teacher forcing. More specifically, we’ll use a `torch.nn.LSTM` to produce a sequence of hidden states that we’ll project to the size of the output vocabulary to predict each target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fairseq.models import FairseqDecoder\n",
    "\n",
    "class SimpleLSTMDecoder(FairseqDecoder):\n",
    "\n",
    "    def __init__(\n",
    "        self, dictionary, encoder_hidden_dim=128, embed_dim=128, hidden_dim=128,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__(dictionary)\n",
    "\n",
    "        # Our decoder will embed the inputs before feeding them to the LSTM.\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            num_embeddings=len(dictionary),\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=dictionary.pad(),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # We'll use a single-layer, unidirectional LSTM for simplicity.\n",
    "        self.lstm = nn.LSTM(\n",
    "            # For the first layer we'll concatenate the Encoder's final hidden\n",
    "            # state with the embedded target tokens.\n",
    "            input_size=encoder_hidden_dim + embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "\n",
    "        # Define the output projection.\n",
    "        self.output_projection = nn.Linear(hidden_dim, len(dictionary))\n",
    "\n",
    "    # During training Decoders are expected to take the entire target sequence\n",
    "    # (shifted right by one position) and produce logits over the vocabulary.\n",
    "    # The *prev_output_tokens* tensor begins with the end-of-sentence symbol,\n",
    "    # ``dictionary.eos()``, followed by the target sequence.\n",
    "    def forward(self, prev_output_tokens, encoder_out):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            prev_output_tokens (LongTensor): previous decoder outputs of shape\n",
    "                `(batch, tgt_len)`, for teacher forcing\n",
    "            encoder_out (Tensor, optional): output from the encoder, used for\n",
    "                encoder-side attention\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - the last decoder layer's output of shape\n",
    "                  `(batch, tgt_len, vocab)`\n",
    "                - the last decoder layer's attention weights of shape\n",
    "                  `(batch, tgt_len, src_len)`\n",
    "        \"\"\"\n",
    "        bsz, tgt_len = prev_output_tokens.size()\n",
    "\n",
    "        # Extract the final hidden state from the Encoder.\n",
    "        final_encoder_hidden = encoder_out['final_hidden']\n",
    "\n",
    "        # Embed the target sequence, which has been shifted right by one\n",
    "        # position and now starts with the end-of-sentence symbol.\n",
    "        x = self.embed_tokens(prev_output_tokens)\n",
    "\n",
    "        # Apply dropout.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Concatenate the Encoder's final hidden state to *every* embedded\n",
    "        # target token.\n",
    "        x = torch.cat(\n",
    "            [x, final_encoder_hidden.unsqueeze(1).expand(bsz, tgt_len, -1)],\n",
    "            dim=2,\n",
    "        )\n",
    "\n",
    "        # Using PackedSequence objects in the Decoder is harder than in the\n",
    "        # Encoder, since the targets are not sorted in descending length order,\n",
    "        # which is a requirement of ``pack_padded_sequence()``. Instead we'll\n",
    "        # feed nn.LSTM directly.\n",
    "        initial_state = (\n",
    "            final_encoder_hidden.unsqueeze(0),  # hidden\n",
    "            torch.zeros_like(final_encoder_hidden).unsqueeze(0),  # cell\n",
    "        )\n",
    "        output, _ = self.lstm(\n",
    "            x.transpose(0, 1),  # convert to shape `(tgt_len, bsz, dim)`\n",
    "            initial_state,\n",
    "        )\n",
    "        x = output.transpose(0, 1)  # convert to shape `(bsz, tgt_len, hidden)`\n",
    "\n",
    "        # Project the outputs to the size of the vocabulary.\n",
    "        x = self.output_projection(x)\n",
    "\n",
    "        # Return the logits and ``None`` for the attention weights\n",
    "        return x, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.models import FairseqEncoderDecoderModel, register_model\n",
    "\n",
    "# Note: the register_model \"decorator\" should immediately precede the\n",
    "# definition of the Model class.\n",
    "\n",
    "@register_model('simple_lstm')\n",
    "class SimpleLSTMModel(FairseqEncoderDecoderModel):\n",
    "\n",
    "    @staticmethod\n",
    "    def add_args(parser):\n",
    "        # Models can override this method to add new command-line arguments.\n",
    "        # Here we'll add some new command-line arguments to configure dropout\n",
    "        # and the dimensionality of the embeddings and hidden states.\n",
    "        parser.add_argument(\n",
    "            '--encoder-embed-dim', type=int, metavar='N',\n",
    "            help='dimensionality of the encoder embeddings',\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--encoder-hidden-dim', type=int, metavar='N',\n",
    "            help='dimensionality of the encoder hidden state',\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--encoder-dropout', type=float, default=0.1,\n",
    "            help='encoder dropout probability',\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--decoder-embed-dim', type=int, metavar='N',\n",
    "            help='dimensionality of the decoder embeddings',\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--decoder-hidden-dim', type=int, metavar='N',\n",
    "            help='dimensionality of the decoder hidden state',\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--decoder-dropout', type=float, default=0.1,\n",
    "            help='decoder dropout probability',\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def build_model(cls, args, task):\n",
    "        # Fairseq initializes models by calling the ``build_model()``\n",
    "        # function. This provides more flexibility, since the returned model\n",
    "        # instance can be of a different type than the one that was called.\n",
    "        # In this case we'll just return a SimpleLSTMModel instance.\n",
    "\n",
    "        # Initialize our Encoder and Decoder.\n",
    "        encoder = SimpleLSTMEncoder(\n",
    "            args=args,\n",
    "            dictionary=task.source_dictionary,\n",
    "            embed_dim=args.encoder_embed_dim,\n",
    "            hidden_dim=args.encoder_hidden_dim,\n",
    "            dropout=args.encoder_dropout,\n",
    "        )\n",
    "        decoder = SimpleLSTMDecoder(\n",
    "            dictionary=task.target_dictionary,\n",
    "            encoder_hidden_dim=args.encoder_hidden_dim,\n",
    "            embed_dim=args.decoder_embed_dim,\n",
    "            hidden_dim=args.decoder_hidden_dim,\n",
    "            dropout=args.decoder_dropout,\n",
    "        )\n",
    "        model = SimpleLSTMModel(encoder, decoder)\n",
    "\n",
    "        # Print the model architecture.\n",
    "        print(model)\n",
    "\n",
    "        return model\n",
    "\n",
    "    # We could override the ``forward()`` if we wanted more control over how\n",
    "    # the encoder and decoder interact, but it's not necessary for this\n",
    "    # tutorial since we can inherit the default implementation provided by\n",
    "    # the FairseqEncoderDecoderModel base class, which looks like:\n",
    "    #\n",
    "    # def forward(self, src_tokens, src_lengths, prev_output_tokens):\n",
    "    #     encoder_out = self.encoder(src_tokens, src_lengths)\n",
    "    #     decoder_out = self.decoder(prev_output_tokens, encoder_out)\n",
    "    #     return decoder_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
