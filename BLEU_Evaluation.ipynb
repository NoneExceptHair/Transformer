{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "MyTf(\n  (transformer_encoder): MyTfEncoder(\n    (layers): ModuleList(\n      (0): MyTfEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=512, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=512, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (activation): ReLU()\n      )\n      (1): MyTfEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=512, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=512, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (activation): ReLU()\n      )\n      (2): MyTfEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=512, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=512, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (activation): ReLU()\n      )\n    )\n    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n  )\n  (transformer_decoder): MyTfDecoder(\n    (layers): ModuleList(\n      (0): MyTfDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=512, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=512, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n        (activation): ReLU()\n      )\n      (1): MyTfDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=512, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=512, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n        (activation): ReLU()\n      )\n      (2): MyTfDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=512, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=512, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n        (activation): ReLU()\n      )\n    )\n    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n  )\n  (generator): Linear(in_features=256, out_features=4555, bias=True)\n  (src_tok_emb): TokenEmbedding(\n    (embedding): Embedding(5374, 256)\n  )\n  (tgt_tok_emb): TokenEmbedding(\n    (embedding): Embedding(4555, 256)\n  )\n  (positional_encoding): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\r\n",
    "from torchtext.data.utils import get_tokenizer\r\n",
    "from src.utils import *\r\n",
    "\r\n",
    "%matplotlib inline\r\n",
    "\r\n",
    "SEED = 42\r\n",
    "torch.manual_seed(SEED)\r\n",
    "torch.cuda.manual_seed(SEED)\r\n",
    "torch.backends.cudnn.deterministic = True\r\n",
    "model_pth = \"./models/\"\r\n",
    "model_name = \"transformer-6-5-1-best\"\r\n",
    "model = torch.load(model_pth + model_name + \".pth.tar\")\r\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7,667,147 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_base = \"./.data/multi30k/task1/raw/\"\r\n",
    "train_pths = ('train.de', 'train.en')\r\n",
    "val_pths = ('val.de', 'val.en')\r\n",
    "test_pths = ('test_2016_flickr.de', 'test_2016_flickr.en')\r\n",
    "train_filepaths = [(pth_base + pth) for pth in train_pths]\r\n",
    "test_filepaths = [(pth_base + pth) for pth in test_pths]\r\n",
    "\r\n",
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\r\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\r\n",
    "\r\n",
    "de_vocab = build_vocab(train_filepaths[0], de_tokenizer, min_freq=3)\r\n",
    "en_vocab = build_vocab(train_filepaths[1], en_tokenizer, min_freq=3)\r\n",
    "\r\n",
    "BOS_IDX = de_vocab['<bos>']\r\n",
    "EOS_IDX = de_vocab['<eos>']\r\n",
    "\r\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5374\n",
      "4555\n"
     ]
    }
   ],
   "source": [
    "print(len(de_vocab))\n",
    "print(len(en_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "' a group of people standing in front of a <unk> sign . '"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, \"eine gruppe von menschen steht vor einem iglu .\", de_vocab, en_vocab, de_tokenizer, BOS_IDX, EOS_IDX, \"greedy\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Reference and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load reference'''\r\n",
    "with open(test_filepaths[0], 'r', encoding='utf8') as f:\r\n",
    "    test_data = f.readlines()\r\n",
    "for i in range(len(test_data)):\r\n",
    "    test_data[i] = test_data[i].rstrip(\"\\n\").lower()\r\n",
    "    \r\n",
    "'''update reference.txt'''\r\n",
    "with open(test_filepaths[1], 'r', encoding='utf8') as f:\r\n",
    "    reference = f.readlines()\r\n",
    "\r\n",
    "for i in range(len(reference)):\r\n",
    "    reference[i] = \" \".join(en_tokenizer(reference[i])).lower()\r\n",
    "\r\n",
    "with open(\"reference.txt\",'w+') as f:\r\n",
    "    f.writelines(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''make predictions'''\r\n",
    "predictions = []\r\n",
    "for data in test_data:\r\n",
    "    temp_trans = translate(model, data.lower(), de_vocab, en_vocab, de_tokenizer, BOS_IDX, EOS_IDX, \"greedy\", device)\r\n",
    "    predictions.append(temp_trans+\"\\n\")\r\n",
    "\r\n",
    "'''update predictions.txt'''\r\n",
    "with open(\"predictions.txt\",'w+') as f:\r\n",
    "    f.writelines(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,pre in enumerate(predictions):\r\n",
    "#     predictions[i] = pre.replace(\" <unk> \",\" \")\r\n",
    "# '''update predictions.txt'''\r\n",
    "# with open(\"predictions.txt\",'w+') as f:\r\n",
    "#     f.writelines(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 37.39, 70.5/46.3/31.2/21.4 (BP=0.972, ratio=0.973, hyp_len=12701, ref_len=13058)\n"
     ]
    }
   ],
   "source": [
    "! perl ./src/multi-bleu.perl -lc reference.txt < predictions.txt\r\n",
    "\r\n",
    "with open(model_pth + model_name + \".txt\",'w+') as f:    \r\n",
    "    f.writelines(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score = 35.27\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "references_corpus  = []\n",
    "candidate_corpus = []\n",
    "for pred,ref in zip(predictions, reference):\n",
    "    temp = pred.rstrip(\" \\n\").split(\" \")\n",
    "    candidate_corpus.append(temp)\n",
    "    temp = ref.rstrip(\" \\n\").split(\" \")\n",
    "    references_corpus.append([temp])\n",
    "bleu_torchtext = bleu_score(candidate_corpus, references_corpus)\n",
    "print(f'BLEU score = {bleu_torchtext*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['five', 'people', 'wearing', 'winter', 'jackets', 'and', 'helmets', 'stand', 'in', 'the', 'snow', ',', 'with', 'snowmobiles', 'in', 'the', 'background', '.']]\n",
      "['', 'five', 'people', 'in', 'winter', 'jackets', 'and', 'helmets', 'are', 'standing', 'in', 'the', 'snow', 'with', '<unk>', 'in', 'the', 'background', '.']\n"
     ]
    }
   ],
   "source": [
    "print(references_corpus[3])\n",
    "print(candidate_corpus[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check BLEU from txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can't open perl script \"./multi-bleu.perl\": No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# with open(model_pth + \"transformer-5-21-7-best\" + \".txt\",'r') as f:    \r\n",
    "#     predictions = f.readlines()\r\n",
    "\r\n",
    "# with open(\"predictions.txt\",'w+') as f:\r\n",
    "#     f.writelines(predictions)\r\n",
    "\r\n",
    "!perl ./multi-bleu.perl -lc reference.txt < predictions.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('ml': conda)",
   "name": "python388jvsc74a57bd07b4d25784dabc0ba7e8cce9dd958aedc39046ba310bddd7030ba48031f69b6c4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}