{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Language Translation with Transformer\n",
    "=====================================\n",
    "\n",
    "This tutorial shows, how to train a translation model from scratch using\n",
    "Transformer. We will be using Multi30k dataset to train a German to English translation model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torchtext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch import Tensor\n",
    "from torch.optim.lr_scheduler import StepLR,LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from utils import *\n",
    "from my_transformer import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_base = \"./.data/multi30k/task1/raw/\"\n",
    "train_pths = ('train.de', 'train.en')\n",
    "val_pths = ('val.de', 'val.en')\n",
    "test_pths = ('test_2016_flickr.de', 'test_2016_flickr.en')\n",
    "\n",
    "train_filepaths = [(pth_base + pth) for pth in train_pths]\n",
    "val_filepaths = [(pth_base + pth) for pth in val_pths]\n",
    "test_filepaths = [(pth_base + pth) for pth in test_pths]\n",
    "\n",
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "de_vocab = build_vocab(train_filepaths[0], de_tokenizer, min_freq=3)\n",
    "en_vocab = build_vocab(train_filepaths[1], en_tokenizer, min_freq=3)\n",
    "    \n",
    "def data_process(filepaths):\n",
    "    raw_de_iter = iter(open(filepaths[0], encoding=\"utf8\"))\n",
    "    raw_en_iter = iter(open(filepaths[1], encoding=\"utf8\"))\n",
    "    data = []\n",
    "    for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "        de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de.lower().rstrip(\"\\n\"))], dtype=torch.long)\n",
    "        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en.lower().rstrip(\"\\n\"))], dtype=torch.long)\n",
    "        data.append((de_tensor_, en_tensor_))\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data = data_process(train_filepaths)\n",
    "val_data = data_process(val_filepaths)\n",
    "test_data = data_process(test_filepaths)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\ntrain size: 29000\nval size: 1014\ntest size: 1000\nde vocab size: 5374\nen vocab size: 4555\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "print(\"train size:\", len(train_data))\n",
    "print(\"val size:\", len(val_data))\n",
    "print(\"test size:\", len(test_data))\n",
    "print(\"de vocab size:\", len(de_vocab))\n",
    "print(\"en vocab size:\", len(en_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = len(de_vocab)\n",
    "TGT_VOCAB_SIZE = len(en_vocab)\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3 # no help, 3 is better\n",
    "NUM_DECODER_LAYERS = 3 # no help, 3 is better\n",
    "EMB_SIZE = 256\n",
    "FFN_HID_DIM = 512\n",
    "NHEAD = 8 # no help, hard converge\n",
    "DROPOUT = 0.1\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "LR_STEP = 30\n",
    "warmup_steps = 4000\n",
    "model_name = \"./models/transformer-5-22-2\"\n",
    "\n",
    "param_list = [model_name,BATCH_SIZE,NUM_ENCODER_LAYERS,NUM_DECODER_LAYERS,EMB_SIZE,FFN_HID_DIM,NHEAD,DROPOUT,NUM_EPOCHS,LEARNING_RATE]\n",
    "with open(\"./parameters/params_history.csv\",\"a+\") as f:\n",
    "#     f.write(\"model_name,BATCH_SIZE,NUM_ENCODER_LAYERS,NUM_DECODER_LAYERS,EMB_SIZE,FFN_HID_DIM,NHEAD,DROPOUT,NUM_EPOCHS,LEARNING_RATE\\n\")\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow(param_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader\n",
    "----------\n",
    "\n",
    "The last torch specific feature we’ll use is the DataLoader, which is\n",
    "easy to use since it takes the data as its first argument. Specifically,\n",
    "as the docs say: DataLoader combines a dataset and a sampler, and\n",
    "provides an iterable over the given dataset. The DataLoader supports\n",
    "both map-style and iterable-style datasets with single- or multi-process\n",
    "loading, customizing loading order and optional automatic batching\n",
    "(collation) and memory pinning.\n",
    "\n",
    "Please pay attention to collate_fn (optional) that merges a list of\n",
    "samples to form a mini-batch of Tensor(s). Used when using batched\n",
    "loading from a map-style dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'BATCH_SIZE' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-99bedc8c4723>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mEOS_IDX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mde_vocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<eos>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgen_collate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPAD_IDX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBOS_IDX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mEOS_IDX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mvalid_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgen_collate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPAD_IDX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBOS_IDX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mEOS_IDX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtest_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgen_collate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPAD_IDX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBOS_IDX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mEOS_IDX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BATCH_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "PAD_IDX = de_vocab['<pad>']\n",
    "BOS_IDX = de_vocab['<bos>']\n",
    "EOS_IDX = de_vocab['<eos>']\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=gen_collate_fn(PAD_IDX,BOS_IDX,EOS_IDX))\n",
    "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=gen_collate_fn(PAD_IDX,BOS_IDX,EOS_IDX))\n",
    "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=gen_collate_fn(PAD_IDX,BOS_IDX,EOS_IDX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer!\n",
    "------------\n",
    "\n",
    "Transformer is a Seq2Seq model introduced in `“Attention is all you\n",
    "need” <https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>`__\n",
    "paper for solving machine translation task. Transformer model consists\n",
    "of an encoder and decoder block each containing fixed number of layers.\n",
    "\n",
    "Encoder processes the input sequence by propogating it, through a series\n",
    "of Multi-head Attention and Feed forward network layers. The output from\n",
    "the Encoder referred to as ``memory``, is fed to the decoder along with\n",
    "target tensors. Encoder and decoder are trained in an end-to-end fashion\n",
    "using teacher forcing technique.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text tokens are represented by using token embeddings. Positional\n",
    "encoding is added to the token embedding to introduce a notion of word\n",
    "order.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a ``subsequent word`` mask to stop a target word from\n",
    "attending to its subsequent words. We also create masks, for masking\n",
    "source and target padding tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model parameters and instantiate model \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, \n",
    "                                 EMB_SIZE, NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,\n",
    "                                 FFN_HID_DIM, DROPOUT)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "lrate = lambda step_num: EMB_SIZE**-0.5 * np.minimum(step_num**-0.5,step_num*warmup_steps**-1.5)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=LR_STEP, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tspan = np.linspace(1,20000)\n",
    "# plt.plot(tspan, lrate(tspan))\n",
    "# plt.xlabel(\"steps\"),plt.ylabel(\"learning rate\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The model has 7,667,147 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'The model has {count_parameters(transformer):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model and Save Checkpoints\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_iter, optimizer):\n",
    "    global steps\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for idx, (src, tgt) in enumerate(train_iter):\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input, PAD_IDX, device)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        \n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        steps += 1\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] = lrate(steps)\n",
    "            \n",
    "        losses += loss.item()\n",
    "    return losses / len(train_iter)\n",
    "\n",
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    for idx, (src, tgt) in (enumerate(valid_iter)):\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input, PAD_IDX, device)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "    return losses / len(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_curve = []\n",
    "val_loss_curve = []\n",
    "min_val_loss = 999\n",
    "steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1, Train loss: 5.497, Val loss: 4.246, Epoch time = 32.498s\n",
      "Epoch: 2, Train loss: 4.006, Val loss: 3.551, Epoch time = 32.219s\n",
      "Epoch: 3, Train loss: 3.521, Val loss: 3.170, Epoch time = 32.383s\n",
      "Epoch: 4, Train loss: 3.197, Val loss: 2.876, Epoch time = 32.443s\n",
      "Epoch: 5, Train loss: 2.942, Val loss: 2.662, Epoch time = 32.283s\n",
      "Epoch: 6, Train loss: 2.738, Val loss: 2.493, Epoch time = 32.581s\n",
      "Epoch: 7, Train loss: 2.569, Val loss: 2.359, Epoch time = 32.416s\n",
      "Epoch: 8, Train loss: 2.424, Val loss: 2.244, Epoch time = 32.503s\n",
      "Epoch: 9, Train loss: 2.300, Val loss: 2.153, Epoch time = 32.533s\n",
      "Epoch: 10, Train loss: 2.188, Val loss: 2.070, Epoch time = 32.553s\n",
      "Epoch: 11, Train loss: 2.091, Val loss: 1.997, Epoch time = 32.408s\n",
      "Epoch: 12, Train loss: 2.002, Val loss: 1.941, Epoch time = 32.529s\n",
      "Epoch: 13, Train loss: 1.921, Val loss: 1.883, Epoch time = 32.463s\n",
      "Epoch: 14, Train loss: 1.849, Val loss: 1.842, Epoch time = 32.465s\n",
      "Epoch: 15, Train loss: 1.782, Val loss: 1.792, Epoch time = 32.482s\n",
      "Epoch: 16, Train loss: 1.717, Val loss: 1.758, Epoch time = 32.689s\n",
      "Epoch: 17, Train loss: 1.661, Val loss: 1.732, Epoch time = 32.619s\n",
      "Epoch: 18, Train loss: 1.607, Val loss: 1.697, Epoch time = 32.761s\n",
      "Epoch: 19, Train loss: 1.556, Val loss: 1.673, Epoch time = 32.383s\n",
      "Epoch: 20, Train loss: 1.511, Val loss: 1.656, Epoch time = 32.780s\n",
      "Epoch: 21, Train loss: 1.467, Val loss: 1.633, Epoch time = 32.414s\n",
      "Epoch: 22, Train loss: 1.426, Val loss: 1.620, Epoch time = 32.552s\n",
      "Epoch: 23, Train loss: 1.386, Val loss: 1.613, Epoch time = 32.499s\n",
      "Epoch: 24, Train loss: 1.347, Val loss: 1.589, Epoch time = 32.588s\n",
      "Epoch: 25, Train loss: 1.315, Val loss: 1.578, Epoch time = 32.553s\n",
      "Epoch: 26, Train loss: 1.280, Val loss: 1.572, Epoch time = 32.637s\n",
      "Epoch: 27, Train loss: 1.249, Val loss: 1.566, Epoch time = 32.567s\n",
      "Epoch: 28, Train loss: 1.217, Val loss: 1.564, Epoch time = 32.475s\n",
      "Epoch: 29, Train loss: 1.188, Val loss: 1.559, Epoch time = 32.496s\n",
      "Epoch: 30, Train loss: 1.160, Val loss: 1.550, Epoch time = 32.448s\n",
      "Epoch: 31, Train loss: 1.133, Val loss: 1.547, Epoch time = 32.608s\n",
      "Epoch: 32, Train loss: 1.106, Val loss: 1.553, Epoch time = 32.677s\n",
      "Epoch: 33, Train loss: 1.083, Val loss: 1.550, Epoch time = 32.422s\n",
      "Epoch: 34, Train loss: 1.058, Val loss: 1.553, Epoch time = 32.681s\n",
      "Epoch: 35, Train loss: 1.034, Val loss: 1.554, Epoch time = 32.659s\n",
      "Epoch: 36, Train loss: 1.012, Val loss: 1.559, Epoch time = 32.647s\n",
      "Epoch: 37, Train loss: 0.989, Val loss: 1.562, Epoch time = 32.423s\n",
      "Epoch: 38, Train loss: 0.965, Val loss: 1.563, Epoch time = 32.569s\n",
      "Epoch: 39, Train loss: 0.948, Val loss: 1.564, Epoch time = 32.373s\n",
      "Epoch: 40, Train loss: 0.926, Val loss: 1.567, Epoch time = 32.666s\n",
      "Epoch: 41, Train loss: 0.908, Val loss: 1.572, Epoch time = 32.400s\n",
      "Epoch: 42, Train loss: 0.888, Val loss: 1.576, Epoch time = 32.456s\n",
      "Epoch: 43, Train loss: 0.868, Val loss: 1.596, Epoch time = 32.411s\n",
      "Epoch: 44, Train loss: 0.850, Val loss: 1.598, Epoch time = 32.595s\n",
      "Epoch: 45, Train loss: 0.832, Val loss: 1.598, Epoch time = 32.415s\n",
      "Epoch: 46, Train loss: 0.813, Val loss: 1.608, Epoch time = 32.582s\n",
      "Epoch: 47, Train loss: 0.797, Val loss: 1.618, Epoch time = 32.561s\n",
      "Epoch: 48, Train loss: 0.780, Val loss: 1.621, Epoch time = 32.618s\n",
      "Epoch: 49, Train loss: 0.764, Val loss: 1.634, Epoch time = 32.405s\n",
      "Epoch: 50, Train loss: 0.748, Val loss: 1.639, Epoch time = 32.570s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(transformer, train_iter, optimizer)\n",
    "    end_time = time.time()\n",
    "    val_loss = evaluate(transformer, valid_iter)\n",
    "#     scheduler.step()\n",
    "    \n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        transformer.eval()\n",
    "        torch.save(transformer, model_name+\"-best.pth.tar\")\n",
    "        \n",
    "    if epoch % 30 == 0:\n",
    "        transformer.eval()\n",
    "        torch.save(transformer, model_name+\"-ckpt-\"+str(epoch)+\".pth.tar\")\n",
    "        \n",
    "    train_loss_curve.append(train_loss)\n",
    "    val_loss_curve.append(val_loss)\n",
    "\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "min val loss: 1.5471777766942978\n"
     ]
    }
   ],
   "source": [
    "print(\"min val loss:\",min_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 376.240625 262.19625\" width=\"376.240625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-05-22T18:19:43.034800</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 262.19625 \r\nL 376.240625 262.19625 \r\nL 376.240625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 34.240625 224.64 \r\nL 369.040625 224.64 \r\nL 369.040625 7.2 \r\nL 34.240625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <path clip-path=\"url(#p0aa309e0bc)\" d=\"M 49.458807 224.64 \r\nL 49.458807 7.2 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_2\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m1906b1959b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"49.458807\" xlink:href=\"#m1906b1959b\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(46.277557 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_3\">\r\n      <path clip-path=\"url(#p0aa309e0bc)\" d=\"M 111.573835 224.64 \r\nL 111.573835 7.2 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"111.573835\" xlink:href=\"#m1906b1959b\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(105.211335 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_5\">\r\n      <path clip-path=\"url(#p0aa309e0bc)\" d=\"M 173.688862 224.64 \r\nL 173.688862 7.2 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"173.688862\" xlink:href=\"#m1906b1959b\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(167.326362 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_7\">\r\n      <path clip-path=\"url(#p0aa309e0bc)\" d=\"M 235.80389 224.64 \r\nL 235.80389 7.2 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.80389\" xlink:href=\"#m1906b1959b\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(229.44139 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_9\">\r\n      <path clip-path=\"url(#p0aa309e0bc)\" d=\"M 297.918918 224.64 \r\nL 297.918918 7.2 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"297.918918\" xlink:href=\"#m1906b1959b\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(291.556418 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_11\">\r\n      <path clip-path=\"url(#p0aa309e0bc)\" d=\"M 360.033946 224.64 \r\nL 360.033946 7.2 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"360.033946\" xlink:href=\"#m1906b1959b\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(353.671446 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- Epoch -->\r\n     <g transform=\"translate(186.329688 252.916562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 9.8125 72.90625 \r\nL 55.90625 72.90625 \r\nL 55.90625 64.59375 \r\nL 19.671875 64.59375 \r\nL 19.671875 43.015625 \r\nL 54.390625 43.015625 \r\nL 54.390625 34.71875 \r\nL 19.671875 34.71875 \r\nL 19.671875 8.296875 \r\nL 56.78125 8.296875 \r\nL 56.78125 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-69\"/>\r\n       <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-112\"/>\r\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-104\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_13\">\r\n      <path clip-path=\"url(#p0aa309e0bc)\" d=\"M 34.240625 204.24856 \r\nL 369.040625 204.24856 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_14\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m0fcb37fcdb\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m0fcb37fcdb\" y=\"204.24856\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 1 -->\r\n      <g transform=\"translate(20.878125 208.047779)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_15\">\r\n      <path clip-path=\"url(#p0aa309e0bc)\" d=\"M 34.240625 162.626222 \r\nL 369.040625 162.626222 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m0fcb37fcdb\" y=\"162.626222\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(20.878125 166.425441)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_17\">\r\n      <path clip-path=\"url(#p0aa309e0bc)\" d=\"M 34.240625 121.003884 \r\nL 369.040625 121.003884 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_18\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m0fcb37fcdb\" y=\"121.003884\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 3 -->\r\n      <g transform=\"translate(20.878125 124.803103)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_19\">\r\n      <path clip-path=\"url(#p0aa309e0bc)\" d=\"M 34.240625 79.381546 \r\nL 369.040625 79.381546 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_20\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m0fcb37fcdb\" y=\"79.381546\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(20.878125 83.180765)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_21\">\r\n      <path clip-path=\"url(#p0aa309e0bc)\" d=\"M 34.240625 37.759208 \r\nL 369.040625 37.759208 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_22\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m0fcb37fcdb\" y=\"37.759208\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(20.878125 41.558427)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_13\">\r\n     <!-- Loss -->\r\n     <g transform=\"translate(14.798437 126.887187)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 9.8125 72.90625 \r\nL 19.671875 72.90625 \r\nL 19.671875 8.296875 \r\nL 55.171875 8.296875 \r\nL 55.171875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-76\"/>\r\n       <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-76\"/>\r\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"167.244141\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_23\">\r\n    <path clip-path=\"url(#p0aa309e0bc)\" d=\"M 49.458807 17.083636 \r\nL 55.67031 79.116893 \r\nL 61.881812 99.302384 \r\nL 68.093315 112.808203 \r\nL 74.304818 123.423515 \r\nL 80.516321 131.89902 \r\nL 86.727824 138.937123 \r\nL 92.939326 144.984217 \r\nL 99.150829 150.146236 \r\nL 105.362332 154.817638 \r\nL 111.573835 158.838926 \r\nL 117.785337 162.529088 \r\nL 123.99684 165.923834 \r\nL 130.208343 168.922359 \r\nL 136.419846 171.697878 \r\nL 142.631349 174.411507 \r\nL 148.842851 176.725132 \r\nL 155.054354 178.974774 \r\nL 161.265857 181.086789 \r\nL 167.47736 182.989507 \r\nL 173.688862 184.830678 \r\nL 179.900365 186.535664 \r\nL 186.111868 188.171864 \r\nL 192.323371 189.787166 \r\nL 198.534874 191.126949 \r\nL 204.746376 192.612349 \r\nL 210.957879 193.89243 \r\nL 217.169382 195.205223 \r\nL 223.380885 196.43501 \r\nL 229.592388 197.58947 \r\nL 235.80389 198.717902 \r\nL 242.015393 199.825304 \r\nL 248.226896 200.790581 \r\nL 254.438399 201.828455 \r\nL 260.649901 202.835264 \r\nL 266.861404 203.753112 \r\nL 273.072907 204.686745 \r\nL 279.28441 205.698443 \r\nL 285.495913 206.43028 \r\nL 291.707415 207.343471 \r\nL 297.918918 208.078941 \r\nL 304.130421 208.91854 \r\nL 310.341924 209.750265 \r\nL 316.553426 210.479166 \r\nL 322.764929 211.243524 \r\nL 328.976432 212.03818 \r\nL 335.187935 212.709092 \r\nL 341.399438 213.417275 \r\nL 347.61094 214.052629 \r\nL 353.822443 214.756364 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_24\">\r\n    <path clip-path=\"url(#p0aa309e0bc)\" d=\"M 49.458807 69.126122 \r\nL 55.67031 98.06667 \r\nL 61.881812 113.942254 \r\nL 68.093315 126.179437 \r\nL 74.304818 135.065227 \r\nL 80.516321 142.09429 \r\nL 86.727824 147.680016 \r\nL 92.939326 152.477583 \r\nL 99.150829 156.248041 \r\nL 105.362332 159.707914 \r\nL 111.573835 162.744019 \r\nL 117.785337 165.100144 \r\nL 123.99684 167.478069 \r\nL 130.208343 169.220968 \r\nL 136.419846 171.295407 \r\nL 142.631349 172.711468 \r\nL 148.842851 173.778871 \r\nL 155.054354 175.249544 \r\nL 161.265857 176.254489 \r\nL 167.47736 176.95209 \r\nL 173.688862 177.893691 \r\nL 179.900365 178.442326 \r\nL 186.111868 178.717291 \r\nL 192.323371 179.753293 \r\nL 198.534874 180.174422 \r\nL 204.746376 180.433765 \r\nL 210.957879 180.684258 \r\nL 217.169382 180.765844 \r\nL 223.380885 180.996916 \r\nL 229.592388 181.345845 \r\nL 235.80389 181.473742 \r\nL 242.015393 181.220402 \r\nL 248.226896 181.336109 \r\nL 254.438399 181.238097 \r\nL 260.649901 181.203713 \r\nL 266.861404 180.995849 \r\nL 273.072907 180.877413 \r\nL 279.28441 180.802436 \r\nL 285.495913 180.781684 \r\nL 291.707415 180.633323 \r\nL 297.918918 180.444614 \r\nL 304.130421 180.255933 \r\nL 310.341924 179.427404 \r\nL 316.553426 179.357925 \r\nL 322.764929 179.35175 \r\nL 328.976432 178.924616 \r\nL 335.187935 178.508271 \r\nL 341.399438 178.39321 \r\nL 347.61094 177.846635 \r\nL 353.822443 177.631693 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 34.240625 224.64 \r\nL 34.240625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 369.040625 224.64 \r\nL 369.040625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 34.240625 224.64 \r\nL 369.040625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 34.240625 7.2 \r\nL 369.040625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 284.271875 44.55625 \r\nL 362.040625 44.55625 \r\nQ 364.040625 44.55625 364.040625 42.55625 \r\nL 364.040625 14.2 \r\nQ 364.040625 12.2 362.040625 12.2 \r\nL 284.271875 12.2 \r\nQ 282.271875 12.2 282.271875 14.2 \r\nL 282.271875 42.55625 \r\nQ 282.271875 44.55625 284.271875 44.55625 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_25\">\r\n     <path d=\"M 286.271875 20.298437 \r\nL 306.271875 20.298437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_26\"/>\r\n    <g id=\"text_14\">\r\n     <!-- train loss -->\r\n     <g transform=\"translate(314.271875 23.798437)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n       <path id=\"DejaVuSans-32\"/>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"264.550781\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"292.333984\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"353.515625\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"405.615234\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_27\">\r\n     <path d=\"M 286.271875 34.976562 \r\nL 306.271875 34.976562 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_28\"/>\r\n    <g id=\"text_15\">\r\n     <!-- val loss -->\r\n     <g transform=\"translate(314.271875 38.476562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"180.029297\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"207.8125\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"268.994141\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"321.09375\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p0aa309e0bc\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"34.240625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(train_loss_curve)\n",
    "plt.plot(val_loss_curve)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend((\"train loss\",\"val loss\"))\n",
    "plt.savefig(\"./images/\" + model_name.split(sep=\"/\")[-1] + \".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' a group of people standing in front of a <unk> <unk> . '"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "translate(transformer, \"eine gruppe von menschen steht vor einem iglu .\", de_vocab, en_vocab, de_tokenizer, BOS_IDX, EOS_IDX, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.eval()\n",
    "torch.save(transformer, model_name + \".pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''make predictions'''\n",
    "predictions = []\n",
    "'''load reference'''\n",
    "with open(test_filepaths[0], 'r', encoding='utf8') as f:\n",
    "    test_data_ = f.readlines()\n",
    "    \n",
    "for data in test_data_:\n",
    "    temp_trans = translate(transformer, data.lower(), de_vocab, en_vocab, de_tokenizer, BOS_IDX, EOS_IDX, device)\n",
    "    predictions.append(temp_trans[1:-3]+\" . \\n\")\n",
    "\n",
    "'''update predictions.txt'''\n",
    "with open(\"predictions.txt\",'w+') as f:\n",
    "    f.writelines(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,pre in enumerate(predictions):\n",
    "#     predictions[i] = pre.replace(\"<unk>\",\" \")\n",
    "# '''update predictions.txt'''\n",
    "# with open(\"predictions.txt\",'w+') as f:\n",
    "#     f.writelines(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BLEU = 37.37, 69.5/45.0/30.1/20.7 (BP=1.000, ratio=1.002, hyp_len=13084, ref_len=13058)\n"
     ]
    }
   ],
   "source": [
    "! perl ./multi-bleu.perl -lc reference.txt < predictions.txt\n",
    "\n",
    "with open(model_name + \".txt\",'w+') as f:    \n",
    "    f.writelines(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BLEU score = 37.20\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "references_corpus  = []\n",
    "candidate_corpus = []\n",
    "'''update reference.txt'''\n",
    "'''update reference.txt'''\n",
    "with open(test_filepaths[1], 'r', encoding='utf8') as f:\n",
    "    reference = f.readlines()\n",
    "\n",
    "for i in range(len(reference)):\n",
    "    reference[i] = \" \".join(en_tokenizer(reference[i])).lower()\n",
    "\n",
    "for pred,ref in zip(predictions, reference):\n",
    "    temp = pred.rstrip(\" \\n\").split(\" \")\n",
    "    candidate_corpus.append(temp)\n",
    "    temp = ref.rstrip(\" \\n\").split(\" \")\n",
    "    references_corpus.append([temp])\n",
    "bleu_torchtext = bleu_score(candidate_corpus, references_corpus)\n",
    "print(f'BLEU score = {bleu_torchtext*100:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python383jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}