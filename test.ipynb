{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd07b4d25784dabc0ba7e8cce9dd958aedc39046ba310bddd7030ba48031f69b6c4",
   "display_name": "Python 3.8.8 64-bit ('ml': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torchtext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.optim.lr_scheduler import StepLR, LambdaLR\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import io\n",
    "import time\n",
    "from utils import *\n",
    "from my_transformer import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_base = \"./.data/multi30k/task1/raw/\"\n",
    "train_pths = ('train.de', 'train.en')\n",
    "val_pths = ('val.de', 'val.en')\n",
    "test_pths = ('test_2016_flickr.de', 'test_2016_flickr.en')\n",
    "vocab_pths = (\"./vocabs/de_vocab.pth\",\"./vocabs/en_vocab.pth\")\n",
    "\n",
    "train_filepaths = [(pth_base + pth) for pth in train_pths]\n",
    "val_filepaths = [(pth_base + pth) for pth in val_pths]\n",
    "test_filepaths = [(pth_base + pth) for pth in test_pths]\n",
    "\n",
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "de_vocab = build_vocab(train_filepaths[0], de_tokenizer, min_freq=1)\n",
    "en_vocab = build_vocab(train_filepaths[1], en_tokenizer, min_freq=1)\n",
    "    \n",
    "def data_process(filepaths):\n",
    "    raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "    data = []\n",
    "    for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "        de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de.lower().rstrip(\"\\n\"))], dtype=torch.long)\n",
    "        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en.lower().rstrip(\"\\n\"))], dtype=torch.long)\n",
    "        data.append((de_tensor_, en_tensor_))\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data = data_process(train_filepaths)\n",
    "val_data = data_process(val_filepaths)\n",
    "test_data = data_process(test_filepaths)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\ntrain size: 29000\nval size: 1014\ntest size: 1000\nde vocab size: 18669\nen vocab size: 9795\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "print(\"train size:\", len(train_data))\n",
    "print(\"val size:\", len(val_data))\n",
    "print(\"test size:\", len(test_data))\n",
    "print(\"de vocab size:\", len(de_vocab))\n",
    "print(\"en vocab size:\", len(en_vocab))"
   ]
  }
 ]
}